{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Migrating from Spark to BigQuery via Dataproc -- Part 3\n\n* [Part 1](01_spark.ipynb): The original Spark code, now running on Dataproc (lift-and-shift).\n* [Part 2](02_gcs.ipynb): Replace HDFS by Google Cloud Storage. This enables job-specific-clusters. (cloud-native)\n* [Part 3](03_automate.ipynb): Automate everything, so that we can run in a job-specific cluster. (cloud-optimized)\n* [Part 4](04_bigquery.ipynb): Load CSV into BigQuery, use BigQuery. (modernize)\n* [Part 5](05_functions.ipynb): Using Cloud Functions, launch analysis every time there is a new file in the bucket. (serverless)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### Catch up: data to GCS"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "--2020-03-02 13:59:20--  http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\nResolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\nConnecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2144903 (2.0M) [application/x-gzip]\nSaving to: \u2018kddcup.data_10_percent.gz.4\u2019\n\nkddcup.data_10_perc 100%[===================>]   2.04M  4.66MB/s    in 0.4s    \n\n2020-03-02 13:59:21 (4.66 MB/s) - \u2018kddcup.data_10_percent.gz.4\u2019 saved [2144903/2144903]\n\nRequirement already satisfied: google-compute-engine in /opt/conda/anaconda/lib/python3.6/site-packages (2.8.13)\nRequirement already satisfied: setuptools in /opt/conda/anaconda/lib/python3.6/site-packages (from google-compute-engine) (45.2.0.post20200210)\nRequirement already satisfied: distro in /opt/conda/anaconda/lib/python3.6/site-packages (from google-compute-engine) (1.4.0)\nRequirement already satisfied: boto in /opt/conda/anaconda/lib/python3.6/site-packages (from google-compute-engine) (2.48.0)\nCopying file://kddcup.data_10_percent.gz [Content-Type=application/octet-stream]...\nCopying file://kddcup.data_10_percent.gz.1 [Content-Type=application/octet-stream]...\nCopying file://kddcup.data_10_percent.gz.2 [Content-Type=application/octet-stream]...\nCopying file://kddcup.data_10_percent.gz.3 [Content-Type=application/octet-stream]...\n\\ [4 files][  8.2 MiB/  8.2 MiB]                                                \n==> NOTE: You are performing a sequence of gsutil operations that may\nrun significantly faster if you instead use gsutil -m cp ... Please\nsee the -m section under \"gsutil help options\" for further information\nabout when gsutil -m can be advantageous.\n\nCopying file://kddcup.data_10_percent.gz.4 [Content-Type=application/octet-stream]...\n\\ [5 files][ 10.2 MiB/ 10.2 MiB]                                                \nOperation completed over 5 objects/10.2 MiB.                                     \n"}], "source": "# Catch up cell. Run if you did not do previous notebooks of this sequence\n!wget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\nBUCKET='julio_demo'  # CHANGE\n!pip install google-compute-engine\n!gsutil cp kdd* gs://$BUCKET/"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://julio_demo/kddcup.data_10_percent.gz\r\ngs://julio_demo/kddcup.data_10_percent.gz.1\r\ngs://julio_demo/kddcup.data_10_percent.gz.2\r\ngs://julio_demo/kddcup.data_10_percent.gz.3\r\ngs://julio_demo/kddcup.data_10_percent.gz.4\r\n"}], "source": "BUCKET='julio_demo'  # CHANGE\n!gsutil ls gs://$BUCKET/kdd*"}, {"cell_type": "markdown", "metadata": {}, "source": "### Create a Python file\n\nPut all the code in a Python file. We can comment out the display-only code such as ```take()``` and ```show()```\nMake changeable settings like ```BUCKET``` come from sys.args"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing spark_analysis.py\n"}], "source": "%%writefile spark_analysis.py\n\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--bucket\", help=\"bucket for input and output\")\nargs = parser.parse_args()\n\nBUCKET = args.bucket"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nfrom pyspark.sql import SparkSession, SQLContext, Row\n\nspark = SparkSession.builder.appName(\"kdd\").getOrCreate()\nsc = spark.sparkContext\ndata_file = \"gs://{}/kddcup.data_10_percent.gz\".format(BUCKET)\nraw_rdd = sc.textFile(data_file).cache()\n#raw_rdd.take(5)"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\ncsv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\nparsed_rdd = csv_rdd.map(lambda r: Row(\n    duration=int(r[0]), \n    protocol_type=r[1],\n    service=r[2],\n    flag=r[3],\n    src_bytes=int(r[4]),\n    dst_bytes=int(r[5]),\n    wrong_fragment=int(r[7]),\n    urgent=int(r[8]),\n    hot=int(r[9]),\n    num_failed_logins=int(r[10]),\n    num_compromised=int(r[12]),\n    su_attempted=r[14],\n    num_root=int(r[15]),\n    num_file_creations=int(r[16]),\n    label=r[-1]\n    )\n)\n#parsed_rdd.take(5)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nsqlContext = SQLContext(sc)\ndf = sqlContext.createDataFrame(parsed_rdd)\nconnections_by_protocol = df.groupBy('protocol_type').count().orderBy('count', ascending=False)\nconnections_by_protocol.show()"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\ndf.registerTempTable(\"connections\")\nattack_stats = sqlContext.sql(\"\"\"\n                           SELECT \n                             protocol_type, \n                             CASE label\n                               WHEN 'normal.' THEN 'no attack'\n                               ELSE 'attack'\n                             END AS state,\n                             COUNT(*) as total_freq,\n                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n                             ROUND(AVG(duration), 2) as mean_duration,\n                             SUM(num_failed_logins) as total_failed_logins,\n                             SUM(num_compromised) as total_compromised,\n                             SUM(num_file_creations) as total_file_creations,\n                             SUM(su_attempted) as total_root_attempts,\n                             SUM(num_root) as total_root_acceses\n                           FROM connections\n                           GROUP BY protocol_type, state\n                           ORDER BY 3 DESC\n                           \"\"\")\nattack_stats.show()"}, {"cell_type": "code", "execution_count": 9, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Write out report\n\nMake sure to copy the output to GCS so that we can safely delete the cluster. This has to be pure Python, so replace shell commands by equivalent Python code."}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nax[0].get_figure().savefig('report.png');\n#!gsutil rm -rf gs://$BUCKET/sparktobq/\n#!gsutil cp report.png gs://$BUCKET/sparktobq/"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nimport google.cloud.storage as gcs\nbucket = gcs.Client().get_bucket(BUCKET)\nfor blob in bucket.list_blobs(prefix='sparktobq/'):\n    blob.delete()\nbucket.blob('sparktobq/report.png').upload_from_filename('report.png')"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Appending to spark_analysis.py\n"}], "source": "%%writefile -a spark_analysis.py\n\nconnections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n    \"gs://{}/sparktobq/connections_by_protocol\".format(BUCKET))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Test automation\n\nRun it standalone"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing to julio_demo\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n20/03/02 14:02:50 WARN org.apache.spark.util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n20/03/02 14:02:50 WARN org.apache.spark.util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n20/03/02 14:02:50 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n[Stage 0:>                                                          (0 + 0) / 1]20/03/02 14:03:14 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:03:29 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:03:44 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:03:59 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n[Stage 0:>                                                          (0 + 0) / 1]20/03/02 14:04:14 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:04:29 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:04:44 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:04:59 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n[Stage 0:>                                                          (0 + 0) / 1]20/03/02 14:05:14 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:05:29 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:05:44 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:05:59 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n[Stage 0:>                                                          (0 + 0) / 1]20/03/02 14:06:14 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n20/03/02 14:06:29 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n^C\nTraceback (most recent call last):\n  File \"spark_analysis.py\", line 39, in <module>\n    df = sqlContext.createDataFrame(parsed_rdd)\n  File \"/usr/lib/spark/python/pyspark/sql/context.py\", line 307, in createDataFrame\n    return self.sparkSession.createDataFrame(data, schema, samplingRatio, verifySchema)\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 746, in createDataFrame\n    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 390, in _createFromRDD\n    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 361, in _inferSchema\n    first = rdd.first()\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1378, in first\n    rs = self.take(1)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1360, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 1069, in runJob\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/opt/conda/anaconda/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1255, in __call__\n    answer = self.gateway_client.send_command(command)\n  File \"/opt/conda/anaconda/lib/python3.6/site-packages/py4j/java_gateway.py\", line 985, in send_command\n    response = connection.send_command(command)\n  File \"/opt/conda/anaconda/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1152, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n  File \"/opt/conda/anaconda/lib/python3.6/socket.py\", line 586, in readinto\n    return self._sock.recv_into(b)\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 270, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n"}], "source": "BUCKET='julio_demo'  # CHANGE\nprint('Writing to {}'.format(BUCKET))\n!python spark_analysis.py --bucket=$BUCKET"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://julio_demo/sparktobq/\r\ngs://julio_demo/sparktobq/connections_by_protocol/\r\ngs://julio_demo/sparktobq/connections_by_protocol/_SUCCESS\r\ngs://julio_demo/sparktobq/connections_by_protocol/part-00000-36337da4-4544-4c59-b6ad-1e8186239241-c000.csv\r\ngs://julio_demo/sparktobq/connections_by_protocol/part-00001-36337da4-4544-4c59-b6ad-1e8186239241-c000.csv\r\ngs://julio_demo/sparktobq/connections_by_protocol/part-00002-36337da4-4544-4c59-b6ad-1e8186239241-c000.csv\r\ngs://julio_demo/sparktobq/report.png\r\n"}], "source": "!gsutil ls gs://$BUCKET/sparktobq/**"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}