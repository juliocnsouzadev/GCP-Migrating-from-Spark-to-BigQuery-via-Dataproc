{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Migrating from Spark to BigQuery via Dataproc -- Part 5\n\n* [Part 1](01_spark.ipynb): The original Spark code, now running on Dataproc (lift-and-shift).\n* [Part 2](02_gcs.ipynb): Replace HDFS by Google Cloud Storage. This enables job-specific-clusters. (cloud-native)\n* [Part 3](03_automate.ipynb): Automate everything, so that we can run in a job-specific cluster. (cloud-optimized)\n* [Part 4](04_bigquery.ipynb): Load CSV into BigQuery, use BigQuery. (modernize)\n* [Part 5](05_functions.ipynb): Using Cloud Functions, launch analysis every time there is a new file in the bucket. (serverless)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### Catch-up cell"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%bash\nwget http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\ngunzip kddcup.data_10_percent.gz\nBUCKET='julio_demo'  # CHANGE\ngsutil cp kdd* gs://$BUCKET/\nbq mk sparktobq"}, {"cell_type": "markdown", "metadata": {}, "source": "### Create reporting function"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing main.py\n"}], "source": "%%writefile main.py\n\nfrom google.cloud import bigquery\nimport google.cloud.storage as gcs\nimport tempfile\nimport os\n\ndef create_report(BUCKET, gcsfilename, tmpdir):\n    \"\"\"\n    Creates report in gs://BUCKET/ based on contents in gcsfilename (gs://bucket/some/dir/filename)\n    \"\"\"\n    # connect to BigQuery\n    client = bigquery.Client()\n    destination_table = client.get_table('sparktobq.kdd_cup')\n    \n    # Specify table schema. Autodetect is not a good idea for production code\n    job_config = bigquery.LoadJobConfig()\n    schema = [\n        bigquery.SchemaField(\"duration\", \"INT64\"),\n    ]\n    for name in ['protocol_type', 'service', 'flag']:\n        schema.append(bigquery.SchemaField(name, \"STRING\"))\n    for name in 'src_bytes,dst_bytes,wrong_fragment,urgent,hot,num_failed_logins'.split(','):\n        schema.append(bigquery.SchemaField(name, \"INT64\"))\n    schema.append(bigquery.SchemaField(\"unused_10\", \"STRING\"))\n    schema.append(bigquery.SchemaField(\"num_compromised\", \"INT64\"))\n    schema.append(bigquery.SchemaField(\"unused_12\", \"STRING\"))\n    for name in 'su_attempted,num_root,num_file_creations'.split(','):\n        schema.append(bigquery.SchemaField(name, \"INT64\")) \n    for fieldno in range(16, 41):\n        schema.append(bigquery.SchemaField(\"unused_{}\".format(fieldno), \"STRING\"))\n    schema.append(bigquery.SchemaField(\"label\", \"STRING\"))\n    job_config.schema = schema\n\n    # Load CSV data into BigQuery, replacing any rows that were there before\n    job_config.create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED\n    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n    job_config.skip_leading_rows = 0\n    job_config.source_format = bigquery.SourceFormat.CSV\n    load_job = client.load_table_from_uri(gcsfilename, destination_table, job_config=job_config)\n    print(\"Starting LOAD job {} for {}\".format(load_job.job_id, gcsfilename))\n    load_job.result()  # Waits for table load to complete.\n    print(\"Finished LOAD job {}\".format(load_job.job_id))\n    \n    # connections by protocol\n    sql = \"\"\"\n        SELECT COUNT(*) AS count\n        FROM sparktobq.kdd_cup\n        GROUP BY protocol_type\n        ORDER by count ASC    \n    \"\"\"\n    connections_by_protocol = client.query(sql).to_dataframe()\n    connections_by_protocol.to_csv(os.path.join(tmpdir,\"connections_by_protocol.csv\"))\n    print(\"Finished analyzing connections\")\n    \n    # attacks plot\n    sql = \"\"\"\n                            SELECT \n                             protocol_type, \n                             CASE label\n                               WHEN 'normal.' THEN 'no attack'\n                               ELSE 'attack'\n                             END AS state,\n                             COUNT(*) as total_freq,\n                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n                             ROUND(AVG(duration), 2) as mean_duration,\n                             SUM(num_failed_logins) as total_failed_logins,\n                             SUM(num_compromised) as total_compromised,\n                             SUM(num_file_creations) as total_file_creations,\n                             SUM(su_attempted) as total_root_attempts,\n                             SUM(num_root) as total_root_acceses\n                           FROM sparktobq.kdd_cup\n                           GROUP BY protocol_type, state\n                           ORDER BY 3 DESC\n    \"\"\"\n    attack_stats = client.query(sql).to_dataframe()\n    ax = attack_stats.plot.bar(x='protocol_type', subplots=True, figsize=(10,25))\n    ax[0].get_figure().savefig(os.path.join(tmpdir,'report.png'));\n    print(\"Finished analyzing attacks\")\n    \n    bucket = gcs.Client().get_bucket(BUCKET)\n    for blob in bucket.list_blobs(prefix='sparktobq/'):\n        blob.delete()\n    for fname in ['report.png', 'connections_by_protocol.csv']:\n        bucket.blob('sparktobq/{}'.format(fname)).upload_from_filename(os.path.join(tmpdir,fname))\n    print(\"Uploaded report based on {} to {}\".format(gcsfilename, BUCKET))\n\n\ndef bigquery_analysis_cf(data, context):\n    # check that trigger is for a file of interest\n    bucket = data['bucket']\n    name = data['name']\n    if ('kddcup' in name) and not ('gz' in name):\n        filename = 'gs://{}/{}'.format(bucket, data['name'])\n        print(bucket, filename)\n        with tempfile.TemporaryDirectory() as tmpdir:\n            create_report(bucket, filename, tmpdir)"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Writing requirements.txt\n"}], "source": "%%writefile requirements.txt\ngoogle-cloud-bigquery\ngoogle-cloud-storage\npandas\nmatplotlib"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "True\n"}], "source": "# verify that the code in the CF works\nname='kddcup.data_10_percent'\nif 'kddcup' in name and not ('gz' in name):\n    print(True)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Test that the function endpoint works"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Starting LOAD job 695105b4-086e-4334-a392-d78b6618b5e3 for gs://julio_demo/kddcup.data_10_percent\nFinished LOAD job 695105b4-086e-4334-a392-d78b6618b5e3\nFinished analyzing connections\nFinished analyzing attacks\nUploaded report based on gs://julio_demo/kddcup.data_10_percent to julio_demo\n"}], "source": "# test that the function works\nimport main as bq\n\nBUCKET='julio_demo' # CHANGE\ntry:\n    bq.create_report(BUCKET, 'gs://{}/kddcup.data_10_percent'.format(BUCKET), \"/tmp\")\nexcept Exception as e:\n    print(e.errors)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Deploy the cloud function"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!gcloud functions deploy bigquery_analysis_cf --runtime python37 --trigger-resource $BUCKET --trigger-event google.storage.object.finalize"}, {"cell_type": "markdown", "metadata": {}, "source": "## Try it out\n\nCopy the file to the bucket:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Removing gs://julio_demo/sparktobq/connections_by_protocol.csv#1583158529344694...\nRemoving gs://julio_demo/sparktobq/report.png#1583158529209296...               \n/ [2 objects]                                                                   \nOperation completed over 2 objects.                                              \nCopying file://kddcup.data_10_percent [Content-Type=application/octet-stream]...\n- [1 files][ 71.4 MiB/ 71.4 MiB]                                                \nOperation completed over 1 objects/71.4 MiB.                                     \n"}], "source": "!gsutil rm -rf gs://$BUCKET/sparktobq\n!gsutil cp kddcup.data_10_percent gs://$BUCKET/"}, {"cell_type": "markdown", "metadata": {}, "source": "Verify that the Cloud Function is being run. You can do this from the [Cloud Functions](https://console.cloud.google.com/functions/) part of the GCP Console.\n\nOnce the function is complete (in about 30 seconds), see if the output folder contains the report:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!gsutil ls gs://$BUCKET/sparktobq"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}